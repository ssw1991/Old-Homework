{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# MY FIRST JOURNAL: PLANS FOR ASSIGNMENT A"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "So far I have completed a script which will filter the supplied data, based soley on formatting/data type.   From here, I need to paralellize the program.  I have not yet run a profile, however, I do know that the current program runs very slow.  After I succesfuly paralellize it, I will work on a better/more efficient code, if I can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bisect\n",
    "import sys\n",
    "import datetime as dt\n",
    "def date_parse(string):\n",
    "    return dt.datetime.strptime(string,'%Y%m%d:%H:%M:%S.%f')\n",
    "# the dates would not parse naturally, so I wrote my own parser\n",
    "packages=[]\n",
    "f=open('data-big.txt')\n",
    "# This essentially initializing everything I need to do.   However, in the\n",
    "# version, I will not need ot open the file.\n",
    "# Syntax unknow but from here, I will execute the code to determine file pointers\n",
    "# in order to open the file and read the correct spot with each process.\n",
    "# that will be followed by: \n",
    "rows = f.readlines(1000000)\n",
    "\n",
    "noise=[]\n",
    "#opens list to store noise.  I will only be storing dates, as they index the data\n",
    "signaldates=[]\n",
    "# opens list to store signaldates, for a window sort.  However the requirements do not require signal,\n",
    "# as such I may only print noise, and filter noise out of signal on open of the norm\n",
    "# program.  This should free up much memory in this process.\n",
    "a=0\n",
    "while a<1000:\n",
    "    signaldates.append(dt.datetime(1900,1,1,1,1,1,000000))\n",
    "    a=a+1\n",
    "#This fills the signal list with arbitrarily early dates,  necessary for the window function.\n",
    "signal=[]\n",
    "#This initializes a list for signal\n",
    "for row in rows:\n",
    "        package ={}\n",
    "#Readlines read until each /n, as such, each record is a row, within the list of rows\n",
    "#now I need to organize, and test the data\n",
    "        cols=row.split(\",\")\n",
    "#split each row by the columns\n",
    "        try:\n",
    "            package[\"date\"]=date_parse(cols[0])\n",
    "            package['price']=float(cols[1])\n",
    "            package['volume']=int(cols[2].strip('\\n'))\n",
    "#test to ensure that the fields a)exist,  and b)fit the expected form\n",
    "# if they do, I am creating a dictionary.  \n",
    "        except:\n",
    "            package['date']=cols[0]\n",
    "            noise.append(package['date'])\n",
    "            continue\n",
    "#if the fields do not appear as expected, the date is placed in the noise list\n",
    "        if package['volume'] <= 0:\n",
    "            noise.append(package['volume'])\n",
    "            continue\n",
    "        if package['price']<= 0:\n",
    "            noise.append(package['price'])\n",
    "            continue\n",
    "#neither price nor volume can be less then zero,  if the corresponding date is noise\n",
    "        insertion=bisect.bisect_left(signaldates,package['date'])\n",
    "        signaldates.insert(insertion,package['date'])\n",
    "        del signaldates[0]\n",
    "#determines, out of the last 1000 dates, where the appropriate insertion point \n",
    "#for the current record is, places the record in order, and finally, removes the \n",
    "#first record to maintain list lenght of 1000\n",
    "        insertion=len(signal)-(len(signaldates)-insertion)\n",
    "        signal.insert(insertion,package)\n",
    "#determines, how far from the end of the signal list, the current record should be \n",
    "#inserted.  Records emerge sorted by date."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "From here, I will look to use MPI Write Ordered, to ensure records maintain order for the normalization program.  \n",
    "\n",
    "One consideration is to conduct the partial sort in the normalization algorigthm, as I will not need to put it to a file, just hold for processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # Normalization Test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I will conduct the normalization test of the log difference.  One issue was apparent, this is an unequal time series.  First I wanted to resample, but that is because I forgot an important function of log function, specifically, they make the continuous function linear.\n",
    "\n",
    "Take for instant time 0, 1, 2, 3, and 4, and I know price at 0 and 4,\n",
    "I can get an average rate of return by finding solving for r in (1+r)^4.\n",
    "\n",
    "However, logs are simple.  The log return is log(p4)-log(p0) = log(p4/p0)\n",
    "\n",
    "However, had we known prices at each instant, we get\n",
    "\n",
    "log(p1)-log(p0)+log(p2)-log(p1)+log(p3)-log(p2)+log(p4)-log(p3)\n",
    "or \n",
    "log(p1/p0)+log(p2/p1)+log(p3/p2)+log(p4/p3)\n",
    "\n",
    " log(p1*p2*p3*p4/p0*p1*p2*p3) = log(p4/p0)\n",
    " \n",
    " in essance I can take the log of (log(pt)-logp(t-1)/(change in time)\n",
    " to get an average log return over the time period, as we do not get consistant time intervals, the log returns will reflect as if it was a specific time interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-2f5225b6badf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprprice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[1;31m#this creates a variable for the previous price\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mprtime\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdate_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2000-01-01 01:01:01.000000'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[1;31m#this creates a variable for prior time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2f5225b6badf>\u001b[0m in \u001b[0;36mdate_parse\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdate_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[1;32mreturn\u001b[0m \u001b[0mdt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S.%f'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpackages\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/Users/wilso/OneDrive/Grad/Datamining/Big Data datafile/signal1.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[1;31m#this will be an MPI open\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dt' is not defined"
     ]
    }
   ],
   "source": [
    "def date_parse(string):\n",
    "    return dt.datetime.strptime(string,'%Y-%m-%d %H:%M:%S.%f')\n",
    "packages=[]\n",
    "f=open('C:/Users/wilso/OneDrive/Grad/Datamining/Big Data datafile/signal1.txt')\n",
    "#this will be an MPI open\n",
    "rows = f.readlines(100000)\n",
    "logret_list=[]\n",
    "prprice=1\n",
    "#this creates a variable for the previous price\n",
    "prtime=date_parse('2000-01-01 01:01:01.000000')\n",
    "#this creates a variable for prior time\n",
    "for row in rows:\n",
    "        package ={}\n",
    "        cols=row.split(\",\")\n",
    "        try:\n",
    "            package[\"date\"]=date_parse(cols[0])\n",
    "        except:\n",
    "            continue\n",
    "#find current date a price (in relation to record)\n",
    "\n",
    "        package['price']=float(cols[1])\n",
    "       # package['volume']=int(cols[2].strip('\\n'))\n",
    "         \n",
    "        timediff=1000000*(package['date']-prtime).total_seconds()\n",
    "#compute time difference\n",
    "        try:\n",
    "            \n",
    "            logret=mt.log10(package['price']/prprice)/timediff\n",
    "#try is included as I did not remove duplicates, and cant divide by 0\n",
    "\n",
    "        except: \n",
    "            continue\n",
    "        logret_list.append(logret)\n",
    "        prprice= package['price']\n",
    "        prtime=package[\"date\"]\n",
    "#creates a list of log returns and resets previous price and previous date        \n",
    "        \n",
    "    \n",
    "logret_list=logret_list[1:]\n",
    "#the first value in this list is wrong, as in any differencing computation\n",
    "\n",
    "plt.hist(logret_list, bins=20,range=[-.0008,.0008])\n",
    "\n",
    "#this allowed me, for trouble shooting to see the graph.  While, as in any \n",
    "#financial data, I expected to see a high peak and fat tails,\n",
    "#I witnessed tails that I believe were too fat, and as a result high kurtosis.\n",
    "#my concern is their were extreme values that should have been filtered out.\n",
    "#however, I do not have the requisite expertise to filter out clearly wrong data\n",
    "#without introducing an obvious bias that I cannot justify.\n",
    "print(scipy.stats.describe(logret_list))\n",
    "\n",
    "#this prints statiscal output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
